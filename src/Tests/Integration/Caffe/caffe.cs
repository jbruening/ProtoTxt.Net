// Classes and structures being serialized

// Generated by ProtocolBuffer
// - a pure c# code generation implementation of protocol buffers
// Report bugs to: https://silentorbit.com/protobuf/

// DO NOT EDIT
// This file will be overwritten when CodeGenerator is run.
// To make custom modifications, edit the .proto file and add //:external before the message line
// then write the code and the changes in a separate file.
using System;
using System.Collections.Generic;

namespace Caffe
{
    /// <summary> Specifies the shape (dimensions) of a Blob.</summary>
    public partial class BlobShape
    {
        public List<long> Dim { get; set; }

    }

    public partial class BlobProto
    {
        public Caffe.BlobShape Shape { get; set; }

        public List<float> Data { get; set; }

        public List<float> Diff { get; set; }

        public List<double> DoubleData { get; set; }

        public List<double> DoubleDiff { get; set; }

        /// <summary> 4D dimensions -- deprecated.  Use "shape" instead.</summary>
        public int Num { get; set; }

        public int Channels { get; set; }

        public int Height { get; set; }

        public int Width { get; set; }

    }

    /// <summary>
    /// <para> The BlobProtoVector is simply a way to pass multiple blobproto instances</para>
    /// <para> around.</para>
    /// </summary>
    public partial class BlobProtoVector
    {
        public List<Caffe.BlobProto> Blobs { get; set; }

    }

    public partial class Datum
    {
        public int Channels { get; set; }

        public int Height { get; set; }

        public int Width { get; set; }

        /// <summary> the actual image data, in bytes</summary>
        public byte[] Data { get; set; }

        public int Label { get; set; }

        /// <summary> Optionally, the datum could also hold float data.</summary>
        public List<float> FloatData { get; set; }

        /// <summary> If true data contains an encoded image that need to be decoded</summary>
        public bool Encoded { get; set; }

    }

    public partial class FillerParameter
    {
        public enum VarianceNorm
        {
            FAN_IN = 0,
            FAN_OUT = 1,
            AVERAGE = 2,
        }

        /// <summary> The filler type.</summary>
        public string Type { get; set; }

        public float Value { get; set; }

        /// <summary> the value in constant filler</summary>
        public float Min { get; set; }

        /// <summary> the min value in uniform filler</summary>
        public float Max { get; set; }

        /// <summary> the max value in uniform filler</summary>
        public float Mean { get; set; }

        /// <summary> the mean value in Gaussian filler</summary>
        public float Std { get; set; }

        /// <summary>
        /// <para> the std value in Gaussian filler</para>
        /// <para> The expected number of non-zero output weights for a given input in</para>
        /// <para> Gaussian filler -- the default -1 means don't perform sparsification.</para>
        /// </summary>
        public int Sparse { get; set; }

        public Caffe.FillerParameter.VarianceNorm variance_norm { get; set; }

    }

    public partial class NetParameter
    {
        public string Name { get; set; }

        /// <summary>
        /// <para> consider giving the network a name</para>
        /// <para> The input blobs to the network.</para>
        /// </summary>
        public List<string> Input { get; set; }

        /// <summary> The shape of the input blobs.</summary>
        public List<Caffe.BlobShape> InputShape { get; set; }

        /// <summary>
        /// <para> 4D input dimensions -- deprecated.  Use "shape" instead.</para>
        /// <para> If specified, for each input blob there should be four</para>
        /// <para> values specifying the num, channels, height and width of the input blob.</para>
        /// <para> Thus, there should be a total of (4 * #input) numbers.</para>
        /// </summary>
        public List<int> InputDim { get; set; }

        /// <summary>
        /// <para> Whether the network will force every layer to carry out backward operation.</para>
        /// <para> If set False, then whether to carry out backward is determined</para>
        /// <para> automatically according to the net structure and learning rates.</para>
        /// </summary>
        public bool ForceBackward { get; set; }

        /// <summary>
        /// <para> The current "state" of the network, including the phase, level, and stage.</para>
        /// <para> Some layers may be included/excluded depending on this state and the states</para>
        /// <para> specified in the layers' include and exclude fields.</para>
        /// </summary>
        public Caffe.NetState State { get; set; }

        /// <summary>
        /// <para> Print debugging information about results while running Net::Forward,</para>
        /// <para> Net::Backward, and Net::Update.</para>
        /// </summary>
        public bool DebugInfo { get; set; }

        /// <summary>
        /// <para> The layers that make up the net.  Each of their configurations, including</para>
        /// <para> connectivity and behavior, is specified as a LayerParameter.</para>
        /// </summary>
        public List<Caffe.LayerParameter> Layer { get; set; }

        /// <summary>
        /// <para> ID 100 so layers are printed last.</para>
        /// <para> DEPRECATED: use 'layer' instead.</para>
        /// </summary>
        public List<Caffe.V1LayerParameter> Layers { get; set; }

    }

    /// <summary>
    /// <para> NOTE</para>
    /// <para> Update the next available ID when you add a new SolverParameter field.</para>
    /// <para></para>
    /// <para> SolverParameter next available ID: 41 (last added: type)</para>
    /// </summary>
    public partial class SolverParameter
    {
        public enum SnapshotFormat
        {
            HDF5 = 0,
            BINARYPROTO = 1,
        }

        public enum SolverMode
        {
            CPU = 0,
            GPU = 1,
        }

        public enum SolverType
        {
            SGD = 0,
            NESTEROV = 1,
            ADAGRAD = 2,
            RMSPROP = 3,
            ADADELTA = 4,
            ADAM = 5,
        }

        /// <summary>
        /// <para> Specifying the train and test networks</para>
        /// <para></para>
        /// <para> Exactly one train net must be specified using one of the following fields:</para>
        /// <para>     train_net_param, train_net, net_param, net</para>
        /// <para> One or more test nets may be specified using any of the following fields:</para>
        /// <para>     test_net_param, test_net, net_param, net</para>
        /// <para> If more than one test net field is specified (e.g., both net and</para>
        /// <para> test_net are specified), they will be evaluated in the field order given</para>
        /// <para> above: (1) test_net_param, (2) test_net, (3) net_param/net.</para>
        /// <para> A test_iter must be specified for each test_net.</para>
        /// <para> A test_level and/or a test_stage may also be specified for each test_net.</para>
        /// <para></para>
        /// <para> Proto filename for the train net, possibly combined with one or more</para>
        /// <para> test nets.</para>
        /// </summary>
        public string Net { get; set; }

        /// <summary> Inline train net param, possibly combined with one or more test nets.</summary>
        public Caffe.NetParameter NetParam { get; set; }

        public string TrainNet { get; set; }

        /// <summary> Proto filename for the train net.</summary>
        public List<string> TestNet { get; set; }

        /// <summary> Proto filenames for the test nets.</summary>
        public Caffe.NetParameter TrainNetParam { get; set; }

        /// <summary> Inline train net params.</summary>
        public List<Caffe.NetParameter> TestNetParam { get; set; }

        /// <summary>
        /// <para> Inline test net params.</para>
        /// <para> The states for the train/test nets. Must be unspecified or</para>
        /// <para> specified once per net.</para>
        /// <para></para>
        /// <para> By default, all states will have solver = true;</para>
        /// <para> train_state will have phase = TRAIN,</para>
        /// <para> and all test_state's will have phase = TEST.</para>
        /// <para> Other defaults are set according to the NetState defaults.</para>
        /// </summary>
        public Caffe.NetState TrainState { get; set; }

        public List<Caffe.NetState> TestState { get; set; }

        /// <summary> The number of iterations for each test net.</summary>
        public List<int> TestIter { get; set; }

        /// <summary> The number of iterations between two testing phases.</summary>
        public int TestInterval { get; set; }

        public bool TestComputeLoss { get; set; }

        /// <summary>
        /// <para> If true, run an initial test pass before the first iteration,</para>
        /// <para> ensuring memory availability and printing the starting value of the loss.</para>
        /// </summary>
        public bool TestInitialization { get; set; }

        public float BaseLr { get; set; }

        /// <summary>
        /// <para> The base learning rate</para>
        /// <para> the number of iterations between displaying info. If display = 0, no info</para>
        /// <para> will be displayed.</para>
        /// </summary>
        public int Display { get; set; }

        /// <summary> Display the loss averaged over the last average_loss iterations</summary>
        public int AverageLoss { get; set; }

        public int MaxIter { get; set; }

        /// <summary>
        /// <para> the maximum number of iterations</para>
        /// <para> accumulate gradients over `iter_size` x `batch_size` instances</para>
        /// </summary>
        public int IterSize { get; set; }

        /// <summary>
        /// <para> The learning rate decay policy. The currently implemented learning rate</para>
        /// <para> policies are as follows:</para>
        /// <para>    - fixed: always return base_lr.</para>
        /// <para>    - step: return base_lr * gamma ^ (floor(iter / step))</para>
        /// <para>    - exp: return base_lr * gamma ^ iter</para>
        /// <para>    - inv: return base_lr * (1 + gamma * iter) ^ (- power)</para>
        /// <para>    - multistep: similar to step but it allows non uniform steps defined by</para>
        /// <para>      stepvalue</para>
        /// <para>    - poly: the effective learning rate follows a polynomial decay, to be</para>
        /// <para>      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)</para>
        /// <para>    - sigmoid: the effective learning rate follows a sigmod decay</para>
        /// <para>      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</para>
        /// <para></para>
        /// <para> where base_lr, max_iter, gamma, step, stepvalue and power are defined</para>
        /// <para> in the solver parameter protocol buffer, and iter is the current iteration.</para>
        /// </summary>
        public string LrPolicy { get; set; }

        public float Gamma { get; set; }

        /// <summary> The parameter to compute the learning rate.</summary>
        public float Power { get; set; }

        /// <summary> The parameter to compute the learning rate.</summary>
        public float Momentum { get; set; }

        /// <summary> The momentum value.</summary>
        public float WeightDecay { get; set; }

        /// <summary>
        /// <para> The weight decay.</para>
        /// <para> regularization types supported: L1 and L2</para>
        /// <para> controlled by weight_decay</para>
        /// </summary>
        public string RegularizationType { get; set; }

        /// <summary> the stepsize for learning rate policy "step"</summary>
        public int Stepsize { get; set; }

        /// <summary> the stepsize for learning rate policy "multistep"</summary>
        public List<int> Stepvalue { get; set; }

        /// <summary>
        /// <para> Set clip_gradients to >= 0 to clip parameter gradients to that L2 norm,</para>
        /// <para> whenever their actual L2 norm is larger.</para>
        /// </summary>
        public float ClipGradients { get; set; }

        public int Snapshot { get; set; }

        /// <summary> The snapshot interval</summary>
        public string SnapshotPrefix { get; set; }

        /// <summary>
        /// <para> The prefix for the snapshot.</para>
        /// <para> whether to snapshot diff in the results or not. Snapshotting diff will help</para>
        /// <para> debugging but the final protocol buffer size will be much larger.</para>
        /// </summary>
        public bool SnapshotDiff { get; set; }

        public Caffe.SolverParameter.SnapshotFormat snapshot_format { get; set; }

        public Caffe.SolverParameter.SolverMode solver_mode { get; set; }

        /// <summary> the device_id will that be used in GPU mode. Use device_id = 0 in default.</summary>
        public int DeviceId { get; set; }

        /// <summary>
        /// <para> If non-negative, the seed with which the Solver will initialize the Caffe</para>
        /// <para> random number generator -- useful for reproducible results. Otherwise,</para>
        /// <para> (and by default) initialize using a seed derived from the system clock.</para>
        /// </summary>
        public long RandomSeed { get; set; }

        /// <summary> type of the solver</summary>
        public string Type { get; set; }

        /// <summary> numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</summary>
        public float Delta { get; set; }

        /// <summary> parameters for the Adam solver</summary>
        public float Momentum2 { get; set; }

        /// <summary>
        /// <para> RMSProp decay value</para>
        /// <para> MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</para>
        /// </summary>
        public float RmsDecay { get; set; }

        /// <summary>
        /// <para> If true, print information about the state of the net that may help with</para>
        /// <para> debugging learning problems.</para>
        /// </summary>
        public bool DebugInfo { get; set; }

        /// <summary> If false, don't save a snapshot after training finishes.</summary>
        public bool SnapshotAfterTrain { get; set; }

        /// <summary> DEPRECATED: use type instead of solver_type</summary>
        public Caffe.SolverParameter.SolverType solver_type { get; set; }

    }

    /// <summary> A message that stores the solver snapshots</summary>
    public partial class SolverState
    {
        public int Iter { get; set; }

        /// <summary> The current iteration</summary>
        public string LearnedNet { get; set; }

        /// <summary> The file that stores the learned net.</summary>
        public List<Caffe.BlobProto> History { get; set; }

        /// <summary> The history for sgd solvers</summary>
        public int CurrentStep { get; set; }

    }

    public partial class NetState
    {
        public Caffe.Phase Phase { get; set; }

        public int Level { get; set; }

        public List<string> Stage { get; set; }

    }

    public partial class NetStateRule
    {
        /// <summary>
        /// <para> Set phase to require the NetState have a particular phase (TRAIN or TEST)</para>
        /// <para> to meet this rule.</para>
        /// </summary>
        public Caffe.Phase Phase { get; set; }

        /// <summary>
        /// <para> Set the minimum and/or maximum levels in which the layer should be used.</para>
        /// <para> Leave undefined to meet the rule regardless of level.</para>
        /// </summary>
        public int MinLevel { get; set; }

        public int MaxLevel { get; set; }

        /// <summary>
        /// <para> Customizable sets of stages to include or exclude.</para>
        /// <para> The net must have ALL of the specified stages and NONE of the specified</para>
        /// <para> "not_stage"s to meet the rule.</para>
        /// <para> (Use multiple NetStateRules to specify conjunctions of stages.)</para>
        /// </summary>
        public List<string> Stage { get; set; }

        public List<string> NotStage { get; set; }

    }

    /// <summary>
    /// <para> Specifies training parameters (multipliers on global learning constants,</para>
    /// <para> and the name and other settings used for weight sharing).</para>
    /// </summary>
    public partial class ParamSpec
    {
        public enum DimCheckMode
        {
            /// <summary> STRICT (default) requires that num, channels, height, width each match.</summary>
            STRICT = 0,
            /// <summary> PERMISSIVE requires only the count (num*channels*height*width) to match.</summary>
            PERMISSIVE = 1,
        }

        /// <summary>
        /// <para> The names of the parameter blobs -- useful for sharing parameters among</para>
        /// <para> layers, but never required otherwise.  To share a parameter between two</para>
        /// <para> layers, give it a (non-empty) name.</para>
        /// </summary>
        public string Name { get; set; }

        /// <summary>
        /// <para> Whether to require shared weights to have the same shape, or just the same</para>
        /// <para> count -- defaults to STRICT if unspecified.</para>
        /// </summary>
        public Caffe.ParamSpec.DimCheckMode ShareMode { get; set; }

        /// <summary> The multiplier on the global learning rate for this parameter.</summary>
        public float LrMult { get; set; }

        /// <summary> The multiplier on the global weight decay for this parameter.</summary>
        public float DecayMult { get; set; }

    }

    /// <summary>
    /// <para> NOTE</para>
    /// <para> Update the next available ID when you add a new LayerParameter field.</para>
    /// <para></para>
    /// <para> LayerParameter next available layer-specific ID: 139 (last added: tile_param)</para>
    /// </summary>
    public partial class LayerParameter
    {
        public string Name { get; set; }

        /// <summary> the layer name</summary>
        public string Type { get; set; }

        /// <summary> the layer type</summary>
        public List<string> Bottom { get; set; }

        /// <summary> the name of each bottom blob</summary>
        public List<string> Top { get; set; }

        /// <summary>
        /// <para> the name of each top blob</para>
        /// <para> The train / test phase for computation.</para>
        /// </summary>
        public Caffe.Phase Phase { get; set; }

        /// <summary>
        /// <para> The amount of weight to assign each top blob in the objective.</para>
        /// <para> Each layer assigns a default value, usually of either 0 or 1,</para>
        /// <para> to each top blob.</para>
        /// </summary>
        public List<float> LossWeight { get; set; }

        /// <summary>
        /// <para> Specifies training parameters (multipliers on global learning constants,</para>
        /// <para> and the name and other settings used for weight sharing).</para>
        /// </summary>
        public List<Caffe.ParamSpec> Param { get; set; }

        /// <summary> The blobs containing the numeric parameters of the layer.</summary>
        public List<Caffe.BlobProto> Blobs { get; set; }

        /// <summary>
        /// <para> Specifies on which bottoms the backpropagation should be skipped.</para>
        /// <para> The size must be either 0 or equal to the number of bottoms.</para>
        /// </summary>
        public List<bool> PropagateDown { get; set; }

        /// <summary>
        /// <para> Rules controlling whether and when a layer is included in the network,</para>
        /// <para> based on the current NetState.  You may specify a non-zero number of rules</para>
        /// <para> to include OR exclude, but not both.  If no include or exclude rules are</para>
        /// <para> specified, the layer is always included.  If the current NetState meets</para>
        /// <para> ANY (i.e., one or more) of the specified rules, the layer is</para>
        /// <para> included/excluded.</para>
        /// </summary>
        public List<Caffe.NetStateRule> Include { get; set; }

        public List<Caffe.NetStateRule> Exclude { get; set; }

        /// <summary> Parameters for data pre-processing.</summary>
        public Caffe.TransformationParameter TransformParam { get; set; }

        /// <summary> Parameters shared by loss layers.</summary>
        public Caffe.LossParameter LossParam { get; set; }

        /// <summary>
        /// <para> Layer type-specific parameters.</para>
        /// <para></para>
        /// <para> Note: certain layers may have more than one computational engine</para>
        /// <para> for their implementation. These layers include an Engine type and</para>
        /// <para> engine parameter for selecting the implementation.</para>
        /// <para> The default for the engine is set by the ENGINE switch at compile-time.</para>
        /// </summary>
        public Caffe.AccuracyParameter AccuracyParam { get; set; }

        public Caffe.ArgMaxParameter ArgmaxParam { get; set; }

        public Caffe.ConcatParameter ConcatParam { get; set; }

        public Caffe.ContrastiveLossParameter ContrastiveLossParam { get; set; }

        public Caffe.ConvolutionParameter ConvolutionParam { get; set; }

        public Caffe.DataParameter DataParam { get; set; }

        public Caffe.DropoutParameter DropoutParam { get; set; }

        public Caffe.DummyDataParameter DummyDataParam { get; set; }

        public Caffe.EltwiseParameter EltwiseParam { get; set; }

        public Caffe.EmbedParameter EmbedParam { get; set; }

        public Caffe.ExpParameter ExpParam { get; set; }

        public Caffe.FlattenParameter FlattenParam { get; set; }

        public Caffe.HDF5DataParameter Hdf5DataParam { get; set; }

        public Caffe.HDF5OutputParameter Hdf5OutputParam { get; set; }

        public Caffe.HingeLossParameter HingeLossParam { get; set; }

        public Caffe.ImageDataParameter ImageDataParam { get; set; }

        public Caffe.InfogainLossParameter InfogainLossParam { get; set; }

        public Caffe.InnerProductParameter InnerProductParam { get; set; }

        public Caffe.LogParameter LogParam { get; set; }

        public Caffe.LRNParameter LrnParam { get; set; }

        public Caffe.MemoryDataParameter MemoryDataParam { get; set; }

        public Caffe.MVNParameter MvnParam { get; set; }

        public Caffe.PoolingParameter PoolingParam { get; set; }

        public Caffe.PowerParameter PowerParam { get; set; }

        public Caffe.PReLUParameter PreluParam { get; set; }

        public Caffe.PythonParameter PythonParam { get; set; }

        public Caffe.ReductionParameter ReductionParam { get; set; }

        public Caffe.ReLUParameter ReluParam { get; set; }

        public Caffe.ReshapeParameter ReshapeParam { get; set; }

        public Caffe.SigmoidParameter SigmoidParam { get; set; }

        public Caffe.SoftmaxParameter SoftmaxParam { get; set; }

        public Caffe.SPPParameter SppParam { get; set; }

        public Caffe.SliceParameter SliceParam { get; set; }

        public Caffe.TanHParameter TanhParam { get; set; }

        public Caffe.ThresholdParameter ThresholdParam { get; set; }

        public Caffe.TileParameter TileParam { get; set; }

        public Caffe.WindowDataParameter WindowDataParam { get; set; }

    }

    /// <summary>
    /// <para> Message that stores parameters used to apply transformation</para>
    /// <para> to the data layer's data</para>
    /// </summary>
    public partial class TransformationParameter
    {
        /// <summary>
        /// <para> For data pre-processing, we can do simple scaling and subtracting the</para>
        /// <para> data mean, if provided. Note that the mean subtraction is always carried</para>
        /// <para> out before scaling.</para>
        /// </summary>
        public float Scale { get; set; }

        /// <summary> Specify if we want to randomly mirror data.</summary>
        public bool Mirror { get; set; }

        /// <summary> Specify if we would like to randomly crop an image.</summary>
        public uint CropSize { get; set; }

        /// <summary> mean_file and mean_value cannot be specified at the same time</summary>
        public string MeanFile { get; set; }

        /// <summary>
        /// <para> if specified can be repeated once (would substract it from all the channels)</para>
        /// <para> or can be repeated the same number of times as channels</para>
        /// <para> (would subtract them from the corresponding channel)</para>
        /// </summary>
        public List<float> MeanValue { get; set; }

        /// <summary> Force the decoded image to have 3 color channels.</summary>
        public bool ForceColor { get; set; }

        /// <summary> Force the decoded image to have 1 color channels.</summary>
        public bool ForceGray { get; set; }

    }

    /// <summary> Message that stores parameters shared by loss layers</summary>
    public partial class LossParameter
    {
        /// <summary> If specified, ignore instances with the given label.</summary>
        public int IgnoreLabel { get; set; }

        /// <summary>
        /// <para> If true, normalize each batch across all instances (including spatial</para>
        /// <para> dimesions, but not ignored instances); else, divide by batch size only.</para>
        /// </summary>
        public bool Normalize { get; set; }

    }

    /// <summary>
    /// <para> Messages that store parameters used by individual layer types follow, in</para>
    /// <para> alphabetical order.</para>
    /// </summary>
    public partial class AccuracyParameter
    {
        /// <summary>
        /// <para> When computing accuracy, count as correct by comparing the true label to</para>
        /// <para> the top k scoring classes.  By default, only compare to the top scoring</para>
        /// <para> class (i.e. argmax).</para>
        /// </summary>
        public uint TopK { get; set; }

        /// <summary>
        /// <para> The "label" axis of the prediction blob, whose argmax corresponds to the</para>
        /// <para> predicted label -- may be negative to index from the end (e.g., -1 for the</para>
        /// <para> last axis).  For example, if axis == 1 and the predictions are</para>
        /// <para> (N x C x H x W), the label blob is expected to contain N*H*W ground truth</para>
        /// <para> labels with integer values in {0, 1, ..., C-1}.</para>
        /// </summary>
        public int Axis { get; set; }

        /// <summary> If specified, ignore instances with the given label.</summary>
        public int IgnoreLabel { get; set; }

    }

    public partial class ArgMaxParameter
    {
        /// <summary> If true produce pairs (argmax, maxval)</summary>
        public bool OutMaxVal { get; set; }

        public uint TopK { get; set; }

        /// <summary>
        /// <para> The axis along which to maximise -- may be negative to index from the</para>
        /// <para> end (e.g., -1 for the last axis).</para>
        /// <para> By default ArgMaxLayer maximizes over the flattened trailing dimensions</para>
        /// <para> for each index of the first / num dimension.</para>
        /// </summary>
        public int Axis { get; set; }

    }

    public partial class ConcatParameter
    {
        /// <summary>
        /// <para> The axis along which to concatenate -- may be negative to index from the</para>
        /// <para> end (e.g., -1 for the last axis).  Other axes must have the</para>
        /// <para> same dimension for all the bottom blobs.</para>
        /// <para> By default, ConcatLayer concatenates blobs along the "channels" axis (1).</para>
        /// </summary>
        public int Axis { get; set; }

        /// <summary> DEPRECATED: alias for "axis" -- does not support negative indexing.</summary>
        public uint ConcatDim { get; set; }

    }

    public partial class ContrastiveLossParameter
    {
        /// <summary> margin for dissimilar pair</summary>
        public float Margin { get; set; }

        /// <summary>
        /// <para> The first implementation of this cost did not exactly match the cost of</para>
        /// <para> Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.</para>
        /// <para> legacy_version = false (the default) uses (margin - d)^2 as proposed in the</para>
        /// <para> Hadsell paper. New models should probably use this version.</para>
        /// <para> legacy_version = true uses (margin - d^2). This is kept to support /</para>
        /// <para> reproduce existing models and results</para>
        /// </summary>
        public bool LegacyVersion { get; set; }

    }

    public partial class ConvolutionParameter
    {
        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public uint NumOutput { get; set; }

        /// <summary> The number of outputs for the layer</summary>
        public bool BiasTerm { get; set; }

        /// <summary>
        /// <para> whether to have bias terms</para>
        /// <para> Pad, kernel size, and stride are all given as a single value for equal</para>
        /// <para> dimensions in all spatial dimensions, or once per spatial dimension.</para>
        /// </summary>
        public List<uint> Pad { get; set; }

        /// <summary> The padding size; defaults to 0</summary>
        public List<uint> KernelSize { get; set; }

        /// <summary> The kernel size</summary>
        public List<uint> Stride { get; set; }

        /// <summary>
        /// <para> The stride; defaults to 1</para>
        /// <para> For 2D convolution only, the *_h and *_w versions may also be used to</para>
        /// <para> specify both spatial dimensions.</para>
        /// </summary>
        public uint PadH { get; set; }

        /// <summary> The padding height (2D only)</summary>
        public uint PadW { get; set; }

        /// <summary> The padding width (2D only)</summary>
        public uint KernelH { get; set; }

        /// <summary> The kernel height (2D only)</summary>
        public uint KernelW { get; set; }

        /// <summary> The kernel width (2D only)</summary>
        public uint StrideH { get; set; }

        /// <summary> The stride height (2D only)</summary>
        public uint StrideW { get; set; }

        /// <summary> The stride width (2D only)</summary>
        public uint Group { get; set; }

        /// <summary> The group size for group conv</summary>
        public Caffe.FillerParameter WeightFiller { get; set; }

        /// <summary> The filler for the weight</summary>
        public Caffe.FillerParameter BiasFiller { get; set; }

        public Caffe.ConvolutionParameter.Engine engine { get; set; }

        /// <summary>
        /// <para> The axis to interpret as "channels" when performing convolution.</para>
        /// <para> Preceding dimensions are treated as independent inputs;</para>
        /// <para> succeeding dimensions are treated as "spatial".</para>
        /// <para> With (N, C, H, W) inputs, and axis == 1 (the default), we perform</para>
        /// <para> N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</para>
        /// <para> groups g>1) filters across the spatial axes (H, W) of the input.</para>
        /// <para> With (N, C, D, H, W) inputs, and axis == 1, we perform</para>
        /// <para> N independent 3D convolutions, sliding (C/g)-channels</para>
        /// <para> filters across the spatial axes (D, H, W) of the input.</para>
        /// </summary>
        public int Axis { get; set; }

        /// <summary>
        /// <para> Whether to force use of the general ND convolution, even if a specific</para>
        /// <para> implementation for blobs of the appropriate number of spatial dimensions</para>
        /// <para> is available. (Currently, there is only a 2D-specific convolution</para>
        /// <para> implementation; for input blobs with num_axes != 2, this option is</para>
        /// <para> ignored and the ND implementation will be used.)</para>
        /// </summary>
        public bool ForceNdIm2col { get; set; }

    }

    public partial class DataParameter
    {
        public enum DB
        {
            LEVELDB = 0,
            LMDB = 1,
        }

        /// <summary> Specify the data source.</summary>
        public string Source { get; set; }

        /// <summary> Specify the batch size.</summary>
        public uint BatchSize { get; set; }

        /// <summary>
        /// <para> The rand_skip variable is for the data layer to skip a few data points</para>
        /// <para> to avoid all asynchronous sgd clients to start at the same point. The skip</para>
        /// <para> point would be set as rand_skip * rand(0,1). Note that rand_skip should not</para>
        /// <para> be larger than the number of keys in the database.</para>
        /// <para> DEPRECATED. Each solver accesses a different subset of the database.</para>
        /// </summary>
        public uint RandSkip { get; set; }

        public Caffe.DataParameter.DB Backend { get; set; }

        /// <summary>
        /// <para> DEPRECATED. See TransformationParameter. For data pre-processing, we can do</para>
        /// <para> simple scaling and subtracting the data mean, if provided. Note that the</para>
        /// <para> mean subtraction is always carried out before scaling.</para>
        /// </summary>
        public float Scale { get; set; }

        public string MeanFile { get; set; }

        /// <summary>
        /// <para> DEPRECATED. See TransformationParameter. Specify if we would like to randomly</para>
        /// <para> crop an image.</para>
        /// </summary>
        public uint CropSize { get; set; }

        /// <summary>
        /// <para> DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</para>
        /// <para> data.</para>
        /// </summary>
        public bool Mirror { get; set; }

        /// <summary> Force the encoded image to have 3 color channels</summary>
        public bool ForceEncodedColor { get; set; }

        /// <summary>
        /// <para> Prefetch queue (Number of batches to prefetch to host memory, increase if</para>
        /// <para> data access bandwidth varies).</para>
        /// </summary>
        public uint Prefetch { get; set; }

    }

    public partial class DropoutParameter
    {
        public float DropoutRatio { get; set; }

    }

    /// <summary>
    /// <para> DummyDataLayer fills any number of arbitrarily shaped blobs with random</para>
    /// <para> (or constant) data generated by "Fillers" (see "message FillerParameter").</para>
    /// </summary>
    public partial class DummyDataParameter
    {
        /// <summary>
        /// <para> This layer produces N >= 1 top blobs.  DummyDataParameter must specify 1 or N</para>
        /// <para> shape fields, and 0, 1 or N data_fillers.</para>
        /// <para></para>
        /// <para> If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.</para>
        /// <para> If 1 data_filler is specified, it is applied to all top blobs.  If N are</para>
        /// <para> specified, the ith is applied to the ith top blob.</para>
        /// </summary>
        public List<Caffe.FillerParameter> DataFiller { get; set; }

        public List<Caffe.BlobShape> Shape { get; set; }

        /// <summary> 4D dimensions -- deprecated.  Use "shape" instead.</summary>
        public List<uint> Num { get; set; }

        public List<uint> Channels { get; set; }

        public List<uint> Height { get; set; }

        public List<uint> Width { get; set; }

    }

    public partial class EltwiseParameter
    {
        public enum EltwiseOp
        {
            PROD = 0,
            SUM = 1,
            MAX = 2,
        }

        public Caffe.EltwiseParameter.EltwiseOp Operation { get; set; }

        /// <summary> element-wise operation</summary>
        public List<float> Coeff { get; set; }

        /// <summary>
        /// <para> blob-wise coefficient for SUM operation</para>
        /// <para> Whether to use an asymptotically slower (for >2 inputs) but stabler method</para>
        /// <para> of computing the gradient for the PROD operation. (No effect for SUM op.)</para>
        /// </summary>
        public bool StableProdGrad { get; set; }

    }

    /// <summary> Message that stores parameters used by EmbedLayer</summary>
    public partial class EmbedParameter
    {
        public uint NumOutput { get; set; }

        /// <summary>
        /// <para> The number of outputs for the layer</para>
        /// <para> The input is given as integers to be interpreted as one-hot</para>
        /// <para> vector indices with dimension num_input.  Hence num_input should be</para>
        /// <para> 1 greater than the maximum possible input value.</para>
        /// </summary>
        public uint InputDim { get; set; }

        public bool BiasTerm { get; set; }

        /// <summary> Whether to use a bias term</summary>
        public Caffe.FillerParameter WeightFiller { get; set; }

        /// <summary> The filler for the weight</summary>
        public Caffe.FillerParameter BiasFiller { get; set; }

    }

    /// <summary> Message that stores parameters used by ExpLayer</summary>
    public partial class ExpParameter
    {
        /// <summary>
        /// <para> ExpLayer computes outputs y = base ^ (shift + scale * x), for base > 0.</para>
        /// <para> Or if base is set to the default (-1), base is set to e,</para>
        /// <para> so y = exp(shift + scale * x).</para>
        /// </summary>
        public float Base { get; set; }

        public float Scale { get; set; }

        public float Shift { get; set; }

    }

    /// <summary> Message that stores parameters used by FlattenLayer</summary>
    public partial class FlattenParameter
    {
        /// <summary>
        /// <para> The first axis to flatten: all preceding axes are retained in the output.</para>
        /// <para> May be negative to index from the end (e.g., -1 for the last axis).</para>
        /// </summary>
        public int Axis { get; set; }

        /// <summary>
        /// <para> The last axis to flatten: all following axes are retained in the output.</para>
        /// <para> May be negative to index from the end (e.g., the default -1 for the last</para>
        /// <para> axis).</para>
        /// </summary>
        public int EndAxis { get; set; }

    }

    /// <summary> Message that stores parameters used by HDF5DataLayer</summary>
    public partial class HDF5DataParameter
    {
        /// <summary> Specify the data source.</summary>
        public string Source { get; set; }

        /// <summary> Specify the batch size.</summary>
        public uint BatchSize { get; set; }

        /// <summary>
        /// <para> Specify whether to shuffle the data.</para>
        /// <para> If shuffle == true, the ordering of the HDF5 files is shuffled,</para>
        /// <para> and the ordering of data within any given HDF5 file is shuffled,</para>
        /// <para> but data between different files are not interleaved; all of a file's</para>
        /// <para> data are output (in a random order) before moving onto another file.</para>
        /// </summary>
        public bool Shuffle { get; set; }

    }

    public partial class HDF5OutputParameter
    {
        public string FileName { get; set; }

    }

    public partial class HingeLossParameter
    {
        public enum Norm
        {
            L1 = 1,
            L2 = 2,
        }

        /// <summary> Specify the Norm to use L1 or L2</summary>
        public Caffe.HingeLossParameter.Norm norm { get; set; }

    }

    public partial class ImageDataParameter
    {
        /// <summary> Specify the data source.</summary>
        public string Source { get; set; }

        /// <summary> Specify the batch size.</summary>
        public uint BatchSize { get; set; }

        /// <summary>
        /// <para> The rand_skip variable is for the data layer to skip a few data points</para>
        /// <para> to avoid all asynchronous sgd clients to start at the same point. The skip</para>
        /// <para> point would be set as rand_skip * rand(0,1). Note that rand_skip should not</para>
        /// <para> be larger than the number of keys in the database.</para>
        /// </summary>
        public uint RandSkip { get; set; }

        /// <summary> Whether or not ImageLayer should shuffle the list of files at every epoch.</summary>
        public bool Shuffle { get; set; }

        /// <summary> It will also resize images if new_height or new_width are not zero.</summary>
        public uint NewHeight { get; set; }

        public uint NewWidth { get; set; }

        /// <summary> Specify if the images are color or gray</summary>
        public bool IsColor { get; set; }

        /// <summary>
        /// <para> DEPRECATED. See TransformationParameter. For data pre-processing, we can do</para>
        /// <para> simple scaling and subtracting the data mean, if provided. Note that the</para>
        /// <para> mean subtraction is always carried out before scaling.</para>
        /// </summary>
        public float Scale { get; set; }

        public string MeanFile { get; set; }

        /// <summary>
        /// <para> DEPRECATED. See TransformationParameter. Specify if we would like to randomly</para>
        /// <para> crop an image.</para>
        /// </summary>
        public uint CropSize { get; set; }

        /// <summary>
        /// <para> DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</para>
        /// <para> data.</para>
        /// </summary>
        public bool Mirror { get; set; }

        public string RootFolder { get; set; }

    }

    public partial class InfogainLossParameter
    {
        /// <summary> Specify the infogain matrix source.</summary>
        public string Source { get; set; }

    }

    public partial class InnerProductParameter
    {
        public uint NumOutput { get; set; }

        /// <summary> The number of outputs for the layer</summary>
        public bool BiasTerm { get; set; }

        /// <summary> whether to have bias terms</summary>
        public Caffe.FillerParameter WeightFiller { get; set; }

        /// <summary> The filler for the weight</summary>
        public Caffe.FillerParameter BiasFiller { get; set; }

        /// <summary>
        /// <para> The filler for the bias</para>
        /// <para> The first axis to be lumped into a single inner product computation;</para>
        /// <para> all preceding axes are retained in the output.</para>
        /// <para> May be negative to index from the end (e.g., -1 for the last axis).</para>
        /// </summary>
        public int Axis { get; set; }

    }

    /// <summary> Message that stores parameters used by LogLayer</summary>
    public partial class LogParameter
    {
        /// <summary>
        /// <para> LogLayer computes outputs y = log_base(shift + scale * x), for base > 0.</para>
        /// <para> Or if base is set to the default (-1), base is set to e,</para>
        /// <para> so y = ln(shift + scale * x) = log_e(shift + scale * x)</para>
        /// </summary>
        public float Base { get; set; }

        public float Scale { get; set; }

        public float Shift { get; set; }

    }

    /// <summary> Message that stores parameters used by LRNLayer</summary>
    public partial class LRNParameter
    {
        public enum NormRegion
        {
            ACROSS_CHANNELS = 0,
            WITHIN_CHANNEL = 1,
        }

        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public uint LocalSize { get; set; }

        public float Alpha { get; set; }

        public float Beta { get; set; }

        public Caffe.LRNParameter.NormRegion norm_region { get; set; }

        public float K { get; set; }

        public Caffe.LRNParameter.Engine engine { get; set; }

    }

    public partial class MemoryDataParameter
    {
        public uint BatchSize { get; set; }

        public uint Channels { get; set; }

        public uint Height { get; set; }

        public uint Width { get; set; }

    }

    public partial class MVNParameter
    {
        /// <summary> This parameter can be set to false to normalize mean only</summary>
        public bool NormalizeVariance { get; set; }

        /// <summary> This parameter can be set to true to perform DNN-like MVN</summary>
        public bool AcrossChannels { get; set; }

        /// <summary> Epsilon for not dividing by zero while normalizing variance</summary>
        public float Eps { get; set; }

    }

    public partial class PoolingParameter
    {
        public enum PoolMethod
        {
            MAX = 0,
            AVE = 1,
            STOCHASTIC = 2,
        }

        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public Caffe.PoolingParameter.PoolMethod Pool { get; set; }

        /// <summary>
        /// <para> The pooling method</para>
        /// <para> Pad, kernel size, and stride are all given as a single value for equal</para>
        /// <para> dimensions in height and width or as Y, X pairs.</para>
        /// </summary>
        public uint Pad { get; set; }

        /// <summary> The padding size (equal in Y, X)</summary>
        public uint PadH { get; set; }

        /// <summary> The padding height</summary>
        public uint PadW { get; set; }

        /// <summary> The padding width</summary>
        public uint KernelSize { get; set; }

        /// <summary> The kernel size (square)</summary>
        public uint KernelH { get; set; }

        /// <summary> The kernel height</summary>
        public uint KernelW { get; set; }

        /// <summary> The kernel width</summary>
        public uint Stride { get; set; }

        /// <summary> The stride (equal in Y, X)</summary>
        public uint StrideH { get; set; }

        /// <summary> The stride height</summary>
        public uint StrideW { get; set; }

        public Caffe.PoolingParameter.Engine engine { get; set; }

        /// <summary>
        /// <para> If global_pooling then it will pool over the size of the bottom by doing</para>
        /// <para> kernel_h = bottom->height and kernel_w = bottom->width</para>
        /// </summary>
        public bool GlobalPooling { get; set; }

    }

    public partial class PowerParameter
    {
        /// <summary> PowerLayer computes outputs y = (shift + scale * x) ^ power.</summary>
        public float Power { get; set; }

        public float Scale { get; set; }

        public float Shift { get; set; }

    }

    public partial class PythonParameter
    {
        public string Module { get; set; }

        public string Layer { get; set; }

        /// <summary>
        /// <para> This value is set to the attribute `param_str` of the `PythonLayer` object</para>
        /// <para> in Python before calling the `setup()` method. This could be a number,</para>
        /// <para> string, dictionary in Python dict format, JSON, etc. You may parse this</para>
        /// <para> string in `setup` method and use it in `forward` and `backward`.</para>
        /// </summary>
        public string ParamStr { get; set; }

        /// <summary>
        /// <para> Whether this PythonLayer is shared among worker solvers during data parallelism.</para>
        /// <para> If true, each worker solver sequentially run forward from this layer.</para>
        /// <para> This value should be set true if you are using it as a data layer.</para>
        /// </summary>
        public bool ShareInParallel { get; set; }

    }

    /// <summary> Message that stores parameters used by ReductionLayer</summary>
    public partial class ReductionParameter
    {
        public enum ReductionOp
        {
            SUM = 1,
            ASUM = 2,
            SUMSQ = 3,
            MEAN = 4,
        }

        public Caffe.ReductionParameter.ReductionOp Operation { get; set; }

        /// <summary>
        /// <para> reduction operation</para>
        /// <para> The first axis to reduce to a scalar -- may be negative to index from the</para>
        /// <para> end (e.g., -1 for the last axis).</para>
        /// <para> (Currently, only reduction along ALL "tail" axes is supported; reduction</para>
        /// <para> of axis M through N, where N < num_axes - 1, is unsupported.)</para>
        /// <para> Suppose we have an n-axis bottom Blob with shape:</para>
        /// <para>     (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).</para>
        /// <para> If axis == m, the output Blob will have shape</para>
        /// <para>     (d0, d1, d2, ..., d(m-1)),</para>
        /// <para> and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))</para>
        /// <para> times, each including (dm * d(m+1) * ... * d(n-1)) individual data.</para>
        /// <para> If axis == 0 (the default), the output Blob always has the empty shape</para>
        /// <para> (count 1), performing reduction across the entire input --</para>
        /// <para> often useful for creating new loss functions.</para>
        /// </summary>
        public int Axis { get; set; }

        public float Coeff { get; set; }

    }

    /// <summary> Message that stores parameters used by ReLULayer</summary>
    public partial class ReLUParameter
    {
        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        /// <summary>
        /// <para> Allow non-zero slope for negative inputs to speed up optimization</para>
        /// <para> Described in:</para>
        /// <para> Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013). Rectifier nonlinearities</para>
        /// <para> improve neural network acoustic models. In ICML Workshop on Deep Learning</para>
        /// <para> for Audio, Speech, and Language Processing.</para>
        /// </summary>
        public float NegativeSlope { get; set; }

        public Caffe.ReLUParameter.Engine engine { get; set; }

    }

    public partial class ReshapeParameter
    {
        /// <summary>
        /// <para> Specify the output dimensions. If some of the dimensions are set to 0,</para>
        /// <para> the corresponding dimension from the bottom layer is used (unchanged).</para>
        /// <para> Exactly one dimension may be set to -1, in which case its value is</para>
        /// <para> inferred from the count of the bottom blob and the remaining dimensions.</para>
        /// <para> For example, suppose we want to reshape a 2D blob "input" with shape 2 x 8:</para>
        /// <para></para>
        /// <para>   layer {</para>
        /// <para>     type: "Reshape" bottom: "input" top: "output"</para>
        /// <para>     reshape_param { ... }</para>
        /// <para>   }</para>
        /// <para></para>
        /// <para> If "input" is 2D with shape 2 x 8, then the following reshape_param</para>
        /// <para> specifications are all equivalent, producing a 3D blob "output" with shape</para>
        /// <para> 2 x 2 x 4:</para>
        /// <para></para>
        /// <para>   reshape_param { shape { dim:  2  dim: 2  dim:  4 } }</para>
        /// <para>   reshape_param { shape { dim:  0  dim: 2  dim:  4 } }</para>
        /// <para>   reshape_param { shape { dim:  0  dim: 2  dim: -1 } }</para>
        /// <para>   reshape_param { shape { dim: -1  dim: 0  dim:  2 } }</para>
        /// </summary>
        public Caffe.BlobShape Shape { get; set; }

        /// <summary>
        /// <para> axis and num_axes control the portion of the bottom blob's shape that are</para>
        /// <para> replaced by (included in) the reshape. By default (axis == 0 and</para>
        /// <para> num_axes == -1), the entire bottom blob shape is included in the reshape,</para>
        /// <para> and hence the shape field must specify the entire output shape.</para>
        /// <para></para>
        /// <para> axis may be non-zero to retain some portion of the beginning of the input</para>
        /// <para> shape (and may be negative to index from the end; e.g., -1 to begin the</para>
        /// <para> reshape after the last axis, including nothing in the reshape,</para>
        /// <para> -2 to include only the last axis, etc.).</para>
        /// <para></para>
        /// <para> For example, suppose "input" is a 2D blob with shape 2 x 8.</para>
        /// <para> Then the following ReshapeLayer specifications are all equivalent,</para>
        /// <para> producing a blob "output" with shape 2 x 2 x 4:</para>
        /// <para></para>
        /// <para>   reshape_param { shape { dim: 2  dim: 2  dim: 4 } }</para>
        /// <para>   reshape_param { shape { dim: 2  dim: 4 } axis:  1 }</para>
        /// <para>   reshape_param { shape { dim: 2  dim: 4 } axis: -3 }</para>
        /// <para></para>
        /// <para> num_axes specifies the extent of the reshape.</para>
        /// <para> If num_axes >= 0 (and axis >= 0), the reshape will be performed only on</para>
        /// <para> input axes in the range [axis, axis+num_axes].</para>
        /// <para> num_axes may also be -1, the default, to include all remaining axes</para>
        /// <para> (starting from axis).</para>
        /// <para></para>
        /// <para> For example, suppose "input" is a 2D blob with shape 2 x 8.</para>
        /// <para> Then the following ReshapeLayer specifications are equivalent,</para>
        /// <para> producing a blob "output" with shape 1 x 2 x 8.</para>
        /// <para></para>
        /// <para>   reshape_param { shape { dim:  1  dim: 2  dim:  8 } }</para>
        /// <para>   reshape_param { shape { dim:  1  dim: 2  }  num_axes: 1 }</para>
        /// <para>   reshape_param { shape { dim:  1  }  num_axes: 0 }</para>
        /// <para></para>
        /// <para> On the other hand, these would produce output blob shape 2 x 1 x 8:</para>
        /// <para></para>
        /// <para>   reshape_param { shape { dim: 2  dim: 1  dim: 8  }  }</para>
        /// <para>   reshape_param { shape { dim: 1 }  axis: 1  num_axes: 0 }</para>
        /// </summary>
        public int Axis { get; set; }

        public int NumAxes { get; set; }

    }

    public partial class SigmoidParameter
    {
        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public Caffe.SigmoidParameter.Engine engine { get; set; }

    }

    public partial class SliceParameter
    {
        /// <summary>
        /// <para> The axis along which to slice -- may be negative to index from the end</para>
        /// <para> (e.g., -1 for the last axis).</para>
        /// <para> By default, SliceLayer concatenates blobs along the "channels" axis (1).</para>
        /// </summary>
        public int Axis { get; set; }

        public List<uint> SlicePoint { get; set; }

        /// <summary> DEPRECATED: alias for "axis" -- does not support negative indexing.</summary>
        public uint SliceDim { get; set; }

    }

    /// <summary> Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer</summary>
    public partial class SoftmaxParameter
    {
        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public Caffe.SoftmaxParameter.Engine engine { get; set; }

        /// <summary>
        /// <para> The axis along which to perform the softmax -- may be negative to index</para>
        /// <para> from the end (e.g., -1 for the last axis).</para>
        /// <para> Any other axes will be evaluated as independent softmaxes.</para>
        /// </summary>
        public int Axis { get; set; }

    }

    public partial class TanHParameter
    {
        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public Caffe.TanHParameter.Engine engine { get; set; }

    }

    /// <summary> Message that stores parameters used by TileLayer</summary>
    public partial class TileParameter
    {
        /// <summary> The index of the axis to tile.</summary>
        public int Axis { get; set; }

        /// <summary> The number of copies (tiles) of the blob to output.</summary>
        public int Tiles { get; set; }

    }

    /// <summary> Message that stores parameters used by ThresholdLayer</summary>
    public partial class ThresholdParameter
    {
        public float Threshold { get; set; }

    }

    public partial class WindowDataParameter
    {
        /// <summary> Specify the data source.</summary>
        public string Source { get; set; }

        /// <summary>
        /// <para> For data pre-processing, we can do simple scaling and subtracting the</para>
        /// <para> data mean, if provided. Note that the mean subtraction is always carried</para>
        /// <para> out before scaling.</para>
        /// </summary>
        public float Scale { get; set; }

        public string MeanFile { get; set; }

        /// <summary> Specify the batch size.</summary>
        public uint BatchSize { get; set; }

        /// <summary> Specify if we would like to randomly crop an image.</summary>
        public uint CropSize { get; set; }

        /// <summary> Specify if we want to randomly mirror data.</summary>
        public bool Mirror { get; set; }

        /// <summary> Foreground (object) overlap threshold</summary>
        public float FgThreshold { get; set; }

        /// <summary> Background (non-object) overlap threshold</summary>
        public float BgThreshold { get; set; }

        /// <summary> Fraction of batch that should be foreground objects</summary>
        public float FgFraction { get; set; }

        /// <summary>
        /// <para> Amount of contextual padding to add around a window</para>
        /// <para> (used only by the window_data_layer)</para>
        /// </summary>
        public uint ContextPad { get; set; }

        /// <summary>
        /// <para> Mode for cropping out a detection window</para>
        /// <para> warp: cropped window is warped to a fixed size and aspect ratio</para>
        /// <para> square: the tightest square around the window is cropped</para>
        /// </summary>
        public string CropMode { get; set; }

        /// <summary> cache_images: will load all images in memory for faster access</summary>
        public bool CacheImages { get; set; }

        /// <summary> append root_folder to locate images</summary>
        public string RootFolder { get; set; }

    }

    public partial class SPPParameter
    {
        public enum PoolMethod
        {
            MAX = 0,
            AVE = 1,
            STOCHASTIC = 2,
        }

        public enum Engine
        {
            DEFAULT = 0,
            CAFFE = 1,
            CUDNN = 2,
        }

        public uint PyramidHeight { get; set; }

        public Caffe.SPPParameter.PoolMethod Pool { get; set; }

        public Caffe.SPPParameter.Engine engine { get; set; }

    }

    /// <summary> DEPRECATED: use LayerParameter.</summary>
    public partial class V1LayerParameter
    {
        public enum LayerType
        {
            NONE = 0,
            ABSVAL = 35,
            ACCURACY = 1,
            ARGMAX = 30,
            BNLL = 2,
            CONCAT = 3,
            CONTRASTIVE_LOSS = 37,
            CONVOLUTION = 4,
            DATA = 5,
            DECONVOLUTION = 39,
            DROPOUT = 6,
            DUMMY_DATA = 32,
            EUCLIDEAN_LOSS = 7,
            ELTWISE = 25,
            EXP = 38,
            FLATTEN = 8,
            HDF5_DATA = 9,
            HDF5_OUTPUT = 10,
            HINGE_LOSS = 28,
            IM2COL = 11,
            IMAGE_DATA = 12,
            INFOGAIN_LOSS = 13,
            INNER_PRODUCT = 14,
            LRN = 15,
            MEMORY_DATA = 29,
            MULTINOMIAL_LOGISTIC_LOSS = 16,
            MVN = 34,
            POOLING = 17,
            POWER = 26,
            RELU = 18,
            SIGMOID = 19,
            SIGMOID_CROSS_ENTROPY_LOSS = 27,
            SILENCE = 36,
            SOFTMAX = 20,
            SOFTMAX_LOSS = 21,
            SPLIT = 22,
            SLICE = 33,
            TANH = 23,
            WINDOW_DATA = 24,
            THRESHOLD = 31,
        }

        public enum DimCheckMode
        {
            STRICT = 0,
            PERMISSIVE = 1,
        }

        public List<string> Bottom { get; set; }

        public List<string> Top { get; set; }

        public string Name { get; set; }

        public List<Caffe.NetStateRule> Include { get; set; }

        public List<Caffe.NetStateRule> Exclude { get; set; }

        public Caffe.V1LayerParameter.LayerType Type { get; set; }

        public List<Caffe.BlobProto> Blobs { get; set; }

        public List<string> Param { get; set; }

        public List<Caffe.V1LayerParameter.DimCheckMode> BlobShareMode { get; set; }

        public List<float> BlobsLr { get; set; }

        public List<float> WeightDecay { get; set; }

        public List<float> LossWeight { get; set; }

        public Caffe.AccuracyParameter AccuracyParam { get; set; }

        public Caffe.ArgMaxParameter ArgmaxParam { get; set; }

        public Caffe.ConcatParameter ConcatParam { get; set; }

        public Caffe.ContrastiveLossParameter ContrastiveLossParam { get; set; }

        public Caffe.ConvolutionParameter ConvolutionParam { get; set; }

        public Caffe.DataParameter DataParam { get; set; }

        public Caffe.DropoutParameter DropoutParam { get; set; }

        public Caffe.DummyDataParameter DummyDataParam { get; set; }

        public Caffe.EltwiseParameter EltwiseParam { get; set; }

        public Caffe.ExpParameter ExpParam { get; set; }

        public Caffe.HDF5DataParameter Hdf5DataParam { get; set; }

        public Caffe.HDF5OutputParameter Hdf5OutputParam { get; set; }

        public Caffe.HingeLossParameter HingeLossParam { get; set; }

        public Caffe.ImageDataParameter ImageDataParam { get; set; }

        public Caffe.InfogainLossParameter InfogainLossParam { get; set; }

        public Caffe.InnerProductParameter InnerProductParam { get; set; }

        public Caffe.LRNParameter LrnParam { get; set; }

        public Caffe.MemoryDataParameter MemoryDataParam { get; set; }

        public Caffe.MVNParameter MvnParam { get; set; }

        public Caffe.PoolingParameter PoolingParam { get; set; }

        public Caffe.PowerParameter PowerParam { get; set; }

        public Caffe.ReLUParameter ReluParam { get; set; }

        public Caffe.SigmoidParameter SigmoidParam { get; set; }

        public Caffe.SoftmaxParameter SoftmaxParam { get; set; }

        public Caffe.SliceParameter SliceParam { get; set; }

        public Caffe.TanHParameter TanhParam { get; set; }

        public Caffe.ThresholdParameter ThresholdParam { get; set; }

        public Caffe.WindowDataParameter WindowDataParam { get; set; }

        public Caffe.TransformationParameter TransformParam { get; set; }

        public Caffe.LossParameter LossParam { get; set; }

        public Caffe.V0LayerParameter Layer { get; set; }

    }

    /// <summary>
    /// <para> DEPRECATED: V0LayerParameter is the old way of specifying layer parameters</para>
    /// <para> in Caffe.  We keep this message type around for legacy support.</para>
    /// </summary>
    public partial class V0LayerParameter
    {
        public enum PoolMethod
        {
            MAX = 0,
            AVE = 1,
            STOCHASTIC = 2,
        }

        public string Name { get; set; }

        /// <summary> the layer name</summary>
        public string Type { get; set; }

        /// <summary>
        /// <para> the string to specify the layer type</para>
        /// <para> Parameters to specify layers with inner products.</para>
        /// </summary>
        public uint NumOutput { get; set; }

        /// <summary> The number of outputs for the layer</summary>
        public bool Biasterm { get; set; }

        /// <summary> whether to have bias terms</summary>
        public Caffe.FillerParameter WeightFiller { get; set; }

        /// <summary> The filler for the weight</summary>
        public Caffe.FillerParameter BiasFiller { get; set; }

        /// <summary> The filler for the bias</summary>
        public uint Pad { get; set; }

        /// <summary> The padding size</summary>
        public uint Kernelsize { get; set; }

        /// <summary> The kernel size</summary>
        public uint Group { get; set; }

        /// <summary> The group size for group conv</summary>
        public uint Stride { get; set; }

        public Caffe.V0LayerParameter.PoolMethod Pool { get; set; }

        /// <summary> The pooling method</summary>
        public float DropoutRatio { get; set; }

        /// <summary> dropout ratio</summary>
        public uint LocalSize { get; set; }

        /// <summary> for local response norm</summary>
        public float Alpha { get; set; }

        /// <summary> for local response norm</summary>
        public float Beta { get; set; }

        /// <summary> for local response norm</summary>
        public float K { get; set; }

        /// <summary> For data layers, specify the data source</summary>
        public string Source { get; set; }

        /// <summary>
        /// <para> For data pre-processing, we can do simple scaling and subtracting the</para>
        /// <para> data mean, if provided. Note that the mean subtraction is always carried</para>
        /// <para> out before scaling.</para>
        /// </summary>
        public float Scale { get; set; }

        public string Meanfile { get; set; }

        /// <summary> For data layers, specify the batch size.</summary>
        public uint Batchsize { get; set; }

        /// <summary> For data layers, specify if we would like to randomly crop an image.</summary>
        public uint Cropsize { get; set; }

        /// <summary> For data layers, specify if we want to randomly mirror data.</summary>
        public bool Mirror { get; set; }

        /// <summary> The blobs containing the numeric parameters of the layer</summary>
        public List<Caffe.BlobProto> Blobs { get; set; }

        /// <summary>
        /// <para> The ratio that is multiplied on the global learning rate. If you want to</para>
        /// <para> set the learning ratio for one blob, you need to set it for all blobs.</para>
        /// </summary>
        public List<float> BlobsLr { get; set; }

        /// <summary> The weight decay that is multiplied on the global weight decay.</summary>
        public List<float> WeightDecay { get; set; }

        /// <summary>
        /// <para> The rand_skip variable is for the data layer to skip a few data points</para>
        /// <para> to avoid all asynchronous sgd clients to start at the same point. The skip</para>
        /// <para> point would be set as rand_skip * rand(0,1). Note that rand_skip should not</para>
        /// <para> be larger than the number of keys in the database.</para>
        /// </summary>
        public uint RandSkip { get; set; }

        /// <summary>
        /// <para> Fields related to detection (det_*)</para>
        /// <para> foreground (object) overlap threshold</para>
        /// </summary>
        public float DetFgThreshold { get; set; }

        /// <summary> background (non-object) overlap threshold</summary>
        public float DetBgThreshold { get; set; }

        /// <summary> Fraction of batch that should be foreground objects</summary>
        public float DetFgFraction { get; set; }

        /// <summary>
        /// <para> optional bool OBSOLETE_can_clobber = 57 [default = true];</para>
        /// <para> Amount of contextual padding to add around a window</para>
        /// <para> (used only by the window_data_layer)</para>
        /// </summary>
        public uint DetContextPad { get; set; }

        /// <summary>
        /// <para> Mode for cropping out a detection window</para>
        /// <para> warp: cropped window is warped to a fixed size and aspect ratio</para>
        /// <para> square: the tightest square around the window is cropped</para>
        /// </summary>
        public string DetCropMode { get; set; }

        /// <summary> For ReshapeLayer, one needs to specify the new dimensions.</summary>
        public int NewNum { get; set; }

        public int NewChannels { get; set; }

        public int NewHeight { get; set; }

        public int NewWidth { get; set; }

        /// <summary>
        /// <para> Whether or not ImageLayer should shuffle the list of files at every epoch.</para>
        /// <para> It will also resize images if new_height or new_width are not zero.</para>
        /// </summary>
        public bool ShuffleImages { get; set; }

        /// <summary>
        /// <para> For ConcatLayer, one needs to specify the dimension for concatenation, and</para>
        /// <para> the other dimensions must be the same for all the bottom blobs.</para>
        /// <para> By default it will concatenate blobs along the channels dimension.</para>
        /// </summary>
        public uint ConcatDim { get; set; }

        public Caffe.HDF5OutputParameter Hdf5OutputParam { get; set; }

    }

    public partial class PReLUParameter
    {
        /// <summary>
        /// <para> Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:</para>
        /// <para> Surpassing Human-Level Performance on ImageNet Classification, 2015.</para>
        /// <para> Initial value of a_i. Default is a_i=0.25 for all i.</para>
        /// </summary>
        public Caffe.FillerParameter Filler { get; set; }

        /// <summary> Whether or not slope paramters are shared across channels.</summary>
        public bool ChannelShared { get; set; }

    }

    public enum Phase
    {
        TRAIN = 0,
        TEST = 1,
    }


}
